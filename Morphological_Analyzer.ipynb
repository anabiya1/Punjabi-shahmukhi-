{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e90ad7-5b10-4f51-8311-d06007d32568",
   "metadata": {},
   "source": [
    "# Morphological Analyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98b289fb-ff68-4d5e-9bf3-c0e0ce75f759",
   "metadata": {},
   "source": [
    "## Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "26339610-f61b-4a80-a091-97c315482745",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer \n",
    "\n",
    "# Suppress TensorFlow warnings\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Only show ERROR messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2206e6b7-1e47-4382-9c26-83512f08435d",
   "metadata": {},
   "source": [
    "## Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b006bb50-e567-4d0d-a903-a1bf320d4571",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Surface</th>\n",
       "      <th>Deep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>نہا</td>\n",
       "      <td>نہاوندیاں</td>\n",
       "      <td>نہا , V , Hab , Fem , Pl , 1P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>نہا</td>\n",
       "      <td>نہاندیاں</td>\n",
       "      <td>نہا,V , Hab , Fem , Pl , 2P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>نہا</td>\n",
       "      <td>نہاوندیاں</td>\n",
       "      <td>نہا ,V , Hab , Fem , Pl , 2P</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>نہا</td>\n",
       "      <td>نہا</td>\n",
       "      <td>نہا,V , Comd , Sg , 2P , Hon1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>نہا</td>\n",
       "      <td>نہائیں</td>\n",
       "      <td>نہا ,V , Comd , Sg , 2P , Hon1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Name    Surface                            Deep\n",
       "0  نہا  نہاوندیاں   نہا , V , Hab , Fem , Pl , 1P\n",
       "1  نہا   نہاندیاں     نہا,V , Hab , Fem , Pl , 2P\n",
       "2  نہا  نہاوندیاں    نہا ,V , Hab , Fem , Pl , 2P\n",
       "3  نہا        نہا   نہا,V , Comd , Sg , 2P , Hon1\n",
       "4  نہا     نہائیں  نہا ,V , Comd , Sg , 2P , Hon1"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"./dataset/complete words.xlsx\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ac7b0c-5f82-4755-ae8d-0751b3dce7d9",
   "metadata": {},
   "source": [
    "## Dataframe Shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8cf2c4f6-2dd8-406b-9089-8706ea66a04d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151884, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09474a5b-42f7-41cf-935d-1999c727ab4e",
   "metadata": {},
   "source": [
    "## Dataframe information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "907561aa-7d91-4bee-945a-37609e74bbd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 151884 entries, 0 to 151883\n",
      "Data columns (total 3 columns):\n",
      " #   Column   Non-Null Count   Dtype \n",
      "---  ------   --------------   ----- \n",
      " 0   Name     151876 non-null  object\n",
      " 1   Surface  151882 non-null  object\n",
      " 2   Deep     151884 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 3.5+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114be5ab-cf0a-4281-89a7-f80338e8f1fb",
   "metadata": {},
   "source": [
    "## Drop null values and prepare features, labels and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64381a08-6b8d-452f-aa76-556442fcd5ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    نہاوندیاں\n",
       "1     نہاندیاں\n",
       "2    نہاوندیاں\n",
       "3          نہا\n",
       "4       نہائیں\n",
       "Name: Surface, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Columns are named 'Name', 'Surface', 'Deep'\n",
    "df.columns = ['Name', 'Surface', 'Deep']\n",
    "\n",
    "# Drop duplicates and NaN values\n",
    "df = df.drop_duplicates()\n",
    "df = df.dropna()\n",
    "\n",
    "# Prepare features and labels\n",
    "X_surface = df['Surface']\n",
    "y_deep = df['Deep']\n",
    "\n",
    "# Display the first few entries\n",
    "X_surface.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "487d6dca-d5ab-454b-8d90-200c2649858c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     نہا , V , Hab , Fem , Pl , 1P\n",
       "1       نہا,V , Hab , Fem , Pl , 2P\n",
       "2      نہا ,V , Hab , Fem , Pl , 2P\n",
       "3     نہا,V , Comd , Sg , 2P , Hon1\n",
       "4    نہا ,V , Comd , Sg , 2P , Hon1\n",
       "Name: Deep, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_deep.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d24db067-822d-4d03-9987-70b93157d60a",
   "metadata": {},
   "source": [
    "## Tokenizing and Padding Surface and Deep Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12e36c5e-8a87-4c72-a3af-6758e0429eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizing the surface words\n",
    "tokenizer_surface = Tokenizer()\n",
    "tokenizer_surface.fit_on_texts(X_surface)\n",
    "X_surface_seq = tokenizer_surface.texts_to_sequences(X_surface)\n",
    "\n",
    "# Padding sequences for surface words\n",
    "max_surface_len = max(len(x) for x in X_surface_seq)\n",
    "X_surface_pad = pad_sequences(X_surface_seq, maxlen=max_surface_len)\n",
    "\n",
    "# Tokenizing the deep representations\n",
    "tokenizer_deep = Tokenizer()\n",
    "tokenizer_deep.fit_on_texts(y_deep)\n",
    "y_deep_seq = tokenizer_deep.texts_to_sequences(y_deep)\n",
    "y_deep_pad = pad_sequences(y_deep_seq, maxlen=1)  # Each surface has a single deep representation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e170b758-b594-462d-bd1d-24c8f5ff4fce",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8b326f-4556-4635-be8b-a153b3347d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (101586, 2), (101586, 1)\n",
      "Test data shape: (25397, 2), (25397, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_surface_pad, y_deep_pad, test_size=0.2, random_state=42)\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}, {y_train.shape}')\n",
    "print(f'Test data shape: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013a895f-432e-4785-995d-aa6bbf45b45e",
   "metadata": {},
   "source": [
    "## Morphological Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad863eaa-0e3a-4fa7-9d35-6d9c16892356",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Surface                                           Analysis\n",
      "0         نہاوندیاں  {'Root': 'نہا', 'POS': 'V', 'Tense': 'Hab', 'G...\n",
      "1          نہاندیاں  {'Root': 'نہا', 'POS': 'V', 'Tense': 'Hab', 'G...\n",
      "2         نہاوندیاں  {'Root': 'نہا', 'POS': 'V', 'Tense': 'Hab', 'G...\n",
      "3               نہا  {'Root': 'نہا', 'POS': 'V', 'Tense': 'Comd', '...\n",
      "4            نہائیں  {'Root': 'نہا', 'POS': 'V', 'Tense': 'Comd', '...\n",
      "...             ...                                                ...\n",
      "151372      چھوٹانے  {'Root': 'چھوٹا', 'POS': 'V + Inf + Mesc + Pl'...\n",
      "151477  مسکراوندہنے  {'Root': 'مسکراوندہ', 'POS': 'V + Inf + Mesc +...\n",
      "151582      ملائمنے  {'Root': 'ملائم', 'POS': 'V + Inf + Mesc + Pl'...\n",
      "151687       کھاسنے  {'Root': 'کھاس', 'POS': 'V + Inf + Mesc + Pl',...\n",
      "151792      چپچپانے  {'Root': 'چپچپا', 'POS': 'V + Inf + Mesc + Pl'...\n",
      "\n",
      "[126983 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Updated function to handle missing values\n",
    "def analyze_morphology(row):\n",
    "    surface_form = row['Surface']\n",
    "    deep_structure = row['Deep']\n",
    "    \n",
    "    # Split the deep structure by commas and strip spaces\n",
    "    deep_tags = [tag.strip() for tag in deep_structure.split(',')]\n",
    "    \n",
    "    # Initialize the analysis with default values in case tags are missing\n",
    "    analysis = {\n",
    "        'Root': deep_tags[0] if len(deep_tags) > 0 else 'Unknown',   # Default to 'Unknown' if missing\n",
    "        'POS': deep_tags[1] if len(deep_tags) > 1 else 'Unknown',\n",
    "        'Tense': deep_tags[2] if len(deep_tags) > 2 else 'Unknown',\n",
    "        'Gender': deep_tags[3] if len(deep_tags) > 3 else 'Unknown',\n",
    "        'Number': deep_tags[4] if len(deep_tags) > 4 else 'Unknown',\n",
    "        'Person': deep_tags[5] if len(deep_tags) > 5 else 'Unknown'\n",
    "    }\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "# Apply the morphological analysis\n",
    "df['Analysis'] = df.apply(analyze_morphology, axis=1)\n",
    "\n",
    "# Print the results\n",
    "print(df[['Surface', 'Analysis']])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d35df8db-e8a5-424b-b7ea-0aeef02a9113",
   "metadata": {},
   "source": [
    "## Analyze New Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "518a5c33-206c-4b00-8f39-e977f390323d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for 'نہاوندیاں': {'Root': 'نہا', 'POS': 'V', 'Tense': 'Hab', 'Gender': 'Fem', 'Number': 'Pl', 'Person': '1P'}\n"
     ]
    }
   ],
   "source": [
    "# Rule-based morphological analysis for a single new word\n",
    "def analyze_new_word(new_word, df):\n",
    "    # Look up the word in the existing dataset\n",
    "    result = df[df['Surface'] == new_word]\n",
    "    \n",
    "    if not result.empty:\n",
    "        # If the word is found, return the existing analysis\n",
    "        return result['Analysis'].values[0]\n",
    "    else:\n",
    "        # If the word is not found, return a default message or further processing\n",
    "        return \"Word not found in the dataset. Unable to predict.\"\n",
    "\n",
    "# Example usage\n",
    "new_word = 'نہاوندیاں'\n",
    "analysis_result = analyze_new_word(new_word, df)\n",
    "print(f\"Analysis for '{new_word}': {analysis_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b0372eb-33cc-417c-b133-4a5089f60a02",
   "metadata": {},
   "source": [
    "## Rule-based morphological analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "854b7e7d-51a1-4921-a608-e27c18f47554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis for 'نہائیں': {'Root': 'نہا', 'POS': 'V', 'Tense': 'Comd', 'Gender': 'Sg', 'Number': '2P', 'Person': 'Hon1'}\n"
     ]
    }
   ],
   "source": [
    "# Rule-based morphological analysis for a single word\n",
    "def analyze_new_word(new_word, df):\n",
    "    # Look up the word in the existing dataset\n",
    "    result = df[df['Surface'] == new_word]\n",
    "    \n",
    "    if not result.empty:\n",
    "        # Split the analysis components from the 'Deep' column\n",
    "        deep_analysis = result['Deep'].values[0].split(',')\n",
    "        \n",
    "        # Order in 'Deep' column corresponds to:\n",
    "        # [Root, POS, Tense, Gender, Number, Person]\n",
    "        analysis_dict = {\n",
    "            'Root': deep_analysis[0].strip(),\n",
    "            'POS': deep_analysis[1].strip(),\n",
    "            'Tense': deep_analysis[2].strip(),\n",
    "            'Gender': deep_analysis[3].strip(),\n",
    "            'Number': deep_analysis[4].strip(),\n",
    "            'Person': deep_analysis[5].strip()\n",
    "        }\n",
    "        \n",
    "        return analysis_dict\n",
    "    else:\n",
    "        # If the word is not found, return a default message\n",
    "        return \"Word not found in the dataset. Unable to predict.\"\n",
    "\n",
    "# Example usage with a new word\n",
    "new_word = 'نہائیں'\n",
    "analysis_result = analyze_new_word(new_word, df)\n",
    "print(f\"Analysis for '{new_word}': {analysis_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2bb043a-f1c4-4f3e-889a-da714a3fbc45",
   "metadata": {},
   "source": [
    "## Finite State Approach: Tokenization, Lexical, Syntax, and Semantic Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e5df19-207d-4993-ba76-a127d12b0272",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: [1, 2, 3, 4, 5]\n",
      "Word Index: {'کالا': 1, 'کتا': 2, 'تیز': 3, 'بھاگدا': 4, 'اے۔': 5}\n",
      "Identified Tokens: ['کالا', 'کتا', 'تیز', 'بھاگدا', 'اے۔']\n",
      "Tagged Tokens (Syntax Analysis): [('کالا', 'JJ'), ('کتا', 'NNP'), ('تیز', 'NNP'), ('بھاگدا', 'NNP'), ('اے۔', 'NN')]\n",
      "Semantic Analysis: {'کالا': 'JJ', 'کتا': 'NNP', 'تیز': 'NNP', 'بھاگدا': 'NNP', 'اے۔': 'NN'}\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Tokenization using Keras\n",
    "def tokenize_text(input_text):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts([input_text])\n",
    "    tokens = tokenizer.texts_to_sequences([input_text])[0]\n",
    "    return tokens, tokenizer.word_index\n",
    "\n",
    "# Step 2: Lexical Analysis\n",
    "def lexical_analysis(tokens, word_index):\n",
    "    indexed_words = {index: word for word, index in word_index.items()}\n",
    "    identified_tokens = [indexed_words[token] for token in tokens]\n",
    "    return identified_tokens\n",
    "\n",
    "# Step 3: Syntax Analysis using NLTK\n",
    "def syntax_analysis(input_text):\n",
    "    words = word_tokenize(input_text)\n",
    "    tagged = pos_tag(words)  # POS tagging for Punjabi\n",
    "    return tagged\n",
    "\n",
    "# Step 4: Semantic Analysis (simple example)\n",
    "def semantic_analysis(tagged_tokens):\n",
    "    semantics = {}\n",
    "    for word, tag in tagged_tokens:\n",
    "        semantics[word] = tag  # Just storing word and its tag\n",
    "    return semantics\n",
    "\n",
    "# Example input text in Punjabi Shahmukhi\n",
    "input_text = \"کالا کتا تیز بھاگدا اے۔\"\n",
    "\n",
    "# Applying the finite state approach\n",
    "tokens, word_index = tokenize_text(input_text)\n",
    "identified_tokens = lexical_analysis(tokens, word_index)\n",
    "tagged_tokens = syntax_analysis(input_text)\n",
    "semantics = semantic_analysis(tagged_tokens)\n",
    "\n",
    "# Output results\n",
    "print(\"Tokens:\", tokens)\n",
    "print(\"Word Index:\", word_index)\n",
    "print(\"Identified Tokens:\", identified_tokens)\n",
    "print(\"Tagged Tokens (Syntax Analysis):\", tagged_tokens)\n",
    "print(\"Semantic Analysis:\", semantics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85242c19-00ba-4aaf-82b0-2848d91102a1",
   "metadata": {},
   "source": [
    "## Train Bidirectional LSTM (Long Short-Term Memory) model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f64a6de4-3935-4bb0-8e67-dfbd43819250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 68ms/step - accuracy: 0.2302 - loss: 2.1488 - val_accuracy: 0.3525 - val_loss: 1.6190\n",
      "Epoch 2/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 68ms/step - accuracy: 0.4541 - loss: 1.3694 - val_accuracy: 0.3806 - val_loss: 1.5558\n",
      "Epoch 3/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 68ms/step - accuracy: 0.5269 - loss: 1.1261 - val_accuracy: 0.3746 - val_loss: 1.6347\n",
      "Epoch 4/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 72ms/step - accuracy: 0.5441 - loss: 1.0208 - val_accuracy: 0.3585 - val_loss: 1.8262\n",
      "Epoch 5/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 72ms/step - accuracy: 0.5465 - loss: 0.9691 - val_accuracy: 0.3678 - val_loss: 2.0149\n",
      "Epoch 6/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m296s\u001b[0m 93ms/step - accuracy: 0.5465 - loss: 0.9356 - val_accuracy: 0.3435 - val_loss: 2.2267\n",
      "Epoch 7/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m273s\u001b[0m 86ms/step - accuracy: 0.5548 - loss: 0.9121 - val_accuracy: 0.3485 - val_loss: 2.3649\n",
      "Epoch 8/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 74ms/step - accuracy: 0.5551 - loss: 0.8976 - val_accuracy: 0.3390 - val_loss: 2.5494\n",
      "Epoch 9/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m272s\u001b[0m 85ms/step - accuracy: 0.5557 - loss: 0.8887 - val_accuracy: 0.3262 - val_loss: 2.8177\n",
      "Epoch 10/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m261s\u001b[0m 82ms/step - accuracy: 0.5593 - loss: 0.8762 - val_accuracy: 0.3435 - val_loss: 2.9550\n",
      "Epoch 11/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m230s\u001b[0m 73ms/step - accuracy: 0.5609 - loss: 0.8703 - val_accuracy: 0.3548 - val_loss: 3.0803\n",
      "Epoch 12/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 74ms/step - accuracy: 0.5642 - loss: 0.8643 - val_accuracy: 0.3441 - val_loss: 3.1585\n",
      "Epoch 13/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m242s\u001b[0m 76ms/step - accuracy: 0.5644 - loss: 0.8631 - val_accuracy: 0.3617 - val_loss: 3.2671\n",
      "Epoch 14/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 69ms/step - accuracy: 0.5691 - loss: 0.8568 - val_accuracy: 0.3559 - val_loss: 3.2566\n",
      "Epoch 15/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 68ms/step - accuracy: 0.5656 - loss: 0.8581 - val_accuracy: 0.3423 - val_loss: 3.4178\n",
      "Epoch 16/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 68ms/step - accuracy: 0.5658 - loss: 0.8586 - val_accuracy: 0.3338 - val_loss: 3.4337\n",
      "Epoch 17/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 67ms/step - accuracy: 0.5658 - loss: 0.8525 - val_accuracy: 0.3494 - val_loss: 3.5505\n",
      "Epoch 18/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 67ms/step - accuracy: 0.5713 - loss: 0.8505 - val_accuracy: 0.3481 - val_loss: 3.4761\n",
      "Epoch 19/50\n",
      "\u001b[1m3175/3175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 67ms/step - accuracy: 0.5676 - loss: 0.8493 - val_accuracy: 0.3137 - val_loss: 3.7294\n",
      "Epoch 20/50\n",
      "\u001b[1m1971/3175\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 66ms/step - accuracy: 0.5702 - loss: 0.8497"
     ]
    }
   ],
   "source": [
    "# Define the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=len(tokenizer_surface.word_index) + 1, output_dim=128, input_length=max_surface_len))\n",
    "model.add(Bidirectional(LSTM(64)))\n",
    "model.add(Dense(len(tokenizer_deep.word_index) + 1, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=32, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d9155a-e967-4483-aef0-b471c096ed6d",
   "metadata": {},
   "source": [
    "## Save Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9970e6d-9e0d-4118-a922-3d47d0afeb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"punjabi_morphological_analyzer.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
